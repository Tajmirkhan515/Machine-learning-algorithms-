{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8266c500-ea2e-4670-9ee5-7de0a6ef66ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I mentioned that the most common supervised learning tasks are regression (predicting values) and classification (predicting classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a15fbf-0728-4d51-9eb0-fba79bf1ae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will be using the MNIST dataset, which is a set of 70,000 small images of digits handwritten by high school students and employees of the US Cen‐sus Bureau. \n",
    "#Each image is labeled with the digit it represents. This set has been studied so much that it is often called the “hello world” of Machine Learning: whenever\n",
    "#people come up with a new classification algorithm they are curious to see how it will perform on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20442503-585a-4e7a-87a7-d2a48351ad78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scikit-Learn provides many helper functions to download popular datasets. MNIST is one of them. The following code fetches the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "410bb896-f0a6-4210-8bf7-bebe2c6c09ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa213e02-04c7-4ecb-81d9-96ad7db98188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist=fetch_openml('mnist_784', version=1)\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b75bd404-2bd5-482b-9903-512a8860dfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=mnist['data'], mnist['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c462b695-0065-4721-9319-beabde4c541b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebc6513d-c3e3-4f49-b7a8-887ed0a38b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54081130-e416-4892-bdee-25bd2148ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#There are 70,000 images, each with 784 features, since each 28 × 28 pixel image has one feature per pixel, \n",
    "#representing intensity from 0 (white) to 255 (black). \n",
    "#To view an image, reshape its feature vector to a 28 × 28 array and display it using Matplotlib’s imshow() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d87658c8-66f8-437f-bd3d-a3a974501319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "some_digit = x.iloc[0].values\n",
    "some_digit_image = some_digit.reshape(28, 28)\n",
    "plt.imshow(some_digit_image, cmap=\"binary\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4350868d-7f92-48df-af35-a5c64b6ccc5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2e46d3f-a565-44cb-b8cd-cfd3d8cad952",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that the label is a string. Most ML algorithms expect numbers, so let’s cast y to integer:\n",
    "import numpy as np\n",
    "y = y.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "255b8c55-16de-4dd2-a8f4-162b2b345057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets split the data into testing and traingin, \n",
    "X_train, X_test, y_train, y_test = x[:60000], x[60000:], y[:60000], y[60000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47cad053-e53e-4bf1-b1f8-061632c9ee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training a Binary Classifier\n",
    "#Let's focus on a simpler task for now: identifying one specific digit, such as the number 5.\n",
    "#This \"5-detector\" will be a binary classifier, which means it can classify between two categories: \n",
    "#images of the digit 5 and images of digits that are not 5. \n",
    "#We'll create target vectors that define these two classes for our classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "495a5928-c6bd-4fe3-824b-f54c964f7830",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_5 = (y_train == 5) # True for all 5s, False for all other digits\n",
    "y_test_5 = (y_test == 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94953ea6-b4e3-4036-b595-d075d7ce5fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A good starting point is the SGDClassifier from Scikit-Learn, ideal for efficiently handling large datasets. \n",
    "#It processes training instances independently, making it suitable for both batch and online learning.\n",
    "#Let's create and train an SGDClassifier on the full training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc483f3b-b828-4807-875a-52516576507f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDClassifier(random_state=42)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "sgd_clf.fit(X_train, y_train_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cfb60a6-828d-4ef7-816b-50ca45877854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but SGDClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_clf.predict([some_digit])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b86cd43-23c2-402f-8e06-2b0432c11314",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The classifier guesses that this image represents a 5 (True). Looks like it guessed right\n",
    "# let's evaluate the model\n",
    "# Performance Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84de9c26-9987-418d-bba5-ce6bf291b95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# different evalution measurement instruments available, let's try\n",
    "#Measuring Accuracy Using Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f9ce79a-13c6-4b1e-b40b-886288361b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9669\n",
      "0.91625\n",
      "0.96785\n"
     ]
    }
   ],
   "source": [
    "#In these cases, you can implement cross-validation\n",
    "#yourself. The following code does roughly the same thing as Scikit-Learn’s\n",
    "#cross_val_score() function, and it prints the same result:\n",
    "\n",
    "X_train = X_train.to_numpy()\n",
    "y_train_5 = y_train_5.to_numpy()\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "skfolds = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "for train_index, test_index in skfolds.split(X_train, y_train_5):\n",
    " clone_clf = clone(sgd_clf)\n",
    " X_train_folds = X_train[train_index]\n",
    " y_train_folds = y_train_5[train_index]\n",
    " X_test_fold = X_train[test_index]\n",
    " y_test_fold = y_train_5[test_index]\n",
    " clone_clf.fit(X_train_folds, y_train_folds)\n",
    " y_pred = clone_clf.predict(X_test_fold)\n",
    " n_correct = sum(y_pred == y_test_fold)\n",
    " print(n_correct / len(y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "469570cb-0679-44ad-8afe-4f5690a201d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using sklearn libaray to evalute\n",
    "#Let's evaluate our SGDClassifier model using K-fold cross-validation with three folds. \n",
    "#This method involves splitting the training set into three folds, making predictions, and evaluating them using a model trained on the other folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e844b88-b796-42ed-a8e0-6c2c6aa41de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.95035, 0.96035, 0.9604 ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d64a7444-5cc9-4d56-9c8d-d69f020eacf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "#A better way to see how well a classifier is working is to use a confusion matrix.\n",
    "#This matrix shows how often each class is mistaken for another class.\n",
    "\n",
    "#To create a confusion matrix, you need a set of predictions to compare with the actual results. \n",
    "#Instead of using the test set (which you should save for final evaluation), \n",
    "#you can use the cross_val_predict() function to make predictions during cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c86f5d0-35d1-4188-9e36-436c7b35c1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b61c47-8b90-466b-b005-fb60ebf72c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now you are ready to get the confusion matrix using the confusion_matrix() func‐tion. \n",
    "#Just pass it the target classes (y_train_5) and the predicted classes(y_train_pred):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5a7220a-76ae-4334-84e7-a917b0f972f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[53892,   687],\n",
       "       [ 1891,  3530]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad51f12c-56a6-466d-9da9-4da14e6e3525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A perfect classifier would have only true positives and true\n",
    "#negatives, so its confusion matrix would have nonzero values only on its main diago‐nal (top left to bottom right):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd942de5-2816-4a37-aa14-8238fb78d031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[54579,     0],\n",
       "       [    0,  5421]], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_perfect_predictions = y_train_5 # pretend we reached perfection\n",
    "confusion_matrix(y_train_5, y_train_perfect_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed3c803-cde9-4755-9c77-853e08dfeaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The confusion matrix gives you a lot of information, but sometimes you may prefer a\n",
    "#more concise metric. An interesting one to look at is the accuracy of the positive pre‐dictions; this is called the precision of the classifier\n",
    "#precision = TP/(TP + FP) TP is the number of true positives, and FP is the number of false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4e9e30bf-eb34-4e8f-8635-db0d657dce44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8370879772350012"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "precision_score(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8d599f90-c67a-4c70-9552-ffc4b4646129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6511713705958311"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c8e334b-a50b-43d7-befc-2052e71f3e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7325171197343846"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To compare two classifiers, it's useful to combine precision and recall into a single metric called the F1 score. \n",
    "#The F1 score is the harmonic mean of precision and recall. Unlike the regular mean, the harmonic mean gives more weight to low values.\n",
    "#This means a classifier will only get a high F1 score if both its precision and recall are high.\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_train_5, y_train_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf2d4d73-fe81-4db1-939b-953825156374",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiclass Classification\n",
    "#Some algorithms, like SGD classifiers, Random Forest classifiers, and naive Bayes classifiers, can handle multiple classes naturally.\n",
    "#Others, like Logistic Regression or Support Vector Machines, are only binary classifiers. However, you can use different strategies to make them work for multiclass classification.\n",
    "#One way to classify digit images into 10 classes (0 to 9) is to train 10 binary classifiers, one for each digit\n",
    "#To classify an image, get the decision score from each classifier and choose the class with the highest score. This is called the one-versus-the-rest (OvR) strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dec112-012a-4acb-9c30-b33c05cc8c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another strategy, called the one-versus-one (OvO) strategy, is to train a binary classifier for every pair of digits: one to distinguish 0s and 1s, another for 0s and 2s, another for 1s and 2s, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc3ea65-1539-4090-a4ee-e4e894862cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some algorithms, like Support Vector Machines, don't handle large training sets well. \n",
    "#For these, OvO is preferred because it's faster to train many classifiers on small sets. \n",
    "#For most binary classifiers, OvR is preferred. Scikit-Learn detects when you use a binary classifier for a multiclass task and automatically runs OvR or OvO based on the algorithm.\n",
    "#Let's try this with a Support Vector Machine using sklearn.svm.SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "316555ee-f6a5-4f19-9fc7-8e2b2c7bdd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7590a53c-ac80-4b84-9af6-2606c3fd0a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but SVC was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5], dtype=uint8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf = SVC()\n",
    "svm_clf.fit(X_train, y_train) # y_train, not y_train_5\n",
    "svm_clf.predict([some_digit])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4f82945-6a63-4da6-b15a-09fed6c9c55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but SVC was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.72501977,  2.72809088,  7.2510018 ,  8.3076379 , -0.31087254,\n",
       "         9.3132482 ,  1.70975103,  2.76765202,  6.23049537,  4.84771048]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_digit_scores = svm_clf.decision_function([some_digit])\n",
    "some_digit_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df68882-da72-437e-a9e9-d9cb53638ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
